<html>
<head>
<style>
@import url('https://fonts.googleapis.com/css?family=Poiret+One');
@import url('https://fonts.googleapis.com/css?family=Josefin+Slab');
body{
background-color: rgb(106, 237, 215);
font-family: 'Josefin Slab', serif;
text-align: center;

}
div{
border-style: solid;
border-color: black;
padding: 20px;
width: 90%;
border-width: 2px;
margin-top: 20px;
border-radius:10px;
margin-bottom: 0;
background-color:rgb(205, 247, 240);
margin: 0 auto;
}
p {
  color: rgb(0,0,0);
  text-align: center;
  font-family: 'Poiret One', cursive;
  font-size: 15.5
}
</style>
</head>
<body>
<h1><strong> Machine Bias: Reflection<strong></h1>
<div>
<p> Inside this article, Machine Bias, we found out about a system called, Northpointe, that inaccurately determines if someone is likely to commit another crime. Even though none of the questions asked during the determination process involve race, somehow black people who have only committed petty crime have been receiving higher, at risk, scores compared to white people with several serious charges. This results in people getting the wrong amount of jail time: either too much, too little,  or sometimes, none at all.</p>
</div>

<div>
<p>While doing some research,  I found that biased algorithms do not only exist in the world of crime. In an article called, Sorry Dave, I [cannot] code that: Al[s]prejudice problem, they discussed how prejudice coding  affects who gets jobs, who gets fired, or who gets credit. I also found out that sometimes, these algorithms are made with intentional bias that do not always concern race. For example, if a company wants to determine its pricing for people they hire,  they will make the code so that they can decide based on where that person lives or even how far they live are from the rival. One interesting idea is that it is important to keep in mind who is writing the code. In the article, it states that if a young, white male is collecting data set, then his code while most likely reflect the view of a young white man. This goes along as well with facial recognition.  Joy Buolamwini, an MIT grad student, was playing with people[s] facial recognition algorithms and realized that white faces were recognized every time, while her face was not being recognized at all. This was because whoever had designed this algorithm did not include a wide range of of skin tones.
</p>
</div>

<div>
<p> As the list of ways  coding can be biased goes on and on, I will stop here. I believe that it honestly sad that even within computer softwares, there is prejudice. Since black people already have this image as criminals in society, this biased algorithm plays a part in that, thus making it harder to live freely. We should not be allowed to input, whether intentional or not, our personal beliefs into coding, because it has no place there. As if the world is not racist and sexiest enough, this just adds to it. This could happen, possibly for that lack of diversity in the coding world. I feel as though if  there are more women and minority coders, then there could potentially be less biased in the coding world.
</p>
</div>
</body>
